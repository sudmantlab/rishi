#

#move data to scratch dir (infiniband) using globus
#to: /global/scratch/users/rdekayne

cd /global/scratch/users/rdekayne/
mkdir -p /global/scratch/users/rdekayne/envs/

module load anaconda3
conda create -p /global/scratch/users/rdekayne/envs/sra
source activate base
conda activate /global/scratch/users/rdekayne/envs/sra
conda install bioconda::sra-tools

#####################################
#####################################
#            DATA
#####################################
#####################################

mkdir -p /global/scratch/users/rdekayne/gorilla_census && cd /global/scratch/users/rdekayne/gorilla_census

mkdir -p /global/scratch/users/rdekayne/gorilla_census/data && cd /global/scratch/users/rdekayne/gorilla_census/data

## 01_download_gorilla_data_01.sh 

#!/bin/bash
#SBATCH --job-name=sra
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl.out # output file
#SBATCH --error=dl.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=4GB # Memory limit of 4GB

mkdir -p ind14_Mkubwa && cd ind14_Mkubwa
fasterq-dump --split-files SRX243452
touch SRX243452.done
fasterq-dump --split-files SRX243453
touch SRX243453.done

## 01_download_gorilla_data_02.sh 

#!/bin/bash
#SBATCH --job-name=sra2
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl2.out # output file
#SBATCH --error=dl2.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=4GB # Memory limit of 4GB

mkdir -p ind15_Kaisi && cd ind15_Kaisi
fasterq-dump --split-files SRX242685
touch SRX242685.done
fasterq-dump --split-files SRX242686
touch SRX242686.done
fasterq-dump --split-files SRX242687
touch SRX242687.done
fasterq-dump --split-files SRX242688
touch SRX242688.done

## 01_download_gorilla_data_03.sh 

#!/bin/bash
#SBATCH --job-name=sra
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl3.out # output file
#SBATCH --error=dl3.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=4GB # Memory limit of 4GB

mkdir -p ind16_Victoria && cd ind16_Victoria
fasterq-dump --split-files SRX243528
touch SRX243528.done
fasterq-dump --split-files SRX243529
touch SRX243529.done
fasterq-dump --split-files SRX243530
touch SRX243530.done
fasterq-dump --split-files SRX243531
touch SRX243531.done
fasterq-dump --split-files SRX243532
touch SRX243532.done
fasterq-dump --split-files SRX243533
touch SRX243533.done

## 01_download_gorilla_data_04.sh ###NOT WORKING##

#!/bin/bash
#SBATCH --job-name=sra
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl4.out # output file
#SBATCH --error=dl4.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=4GB # Memory limit of 4GB

mkdir -p ind01_Maisha && cd ind01_Maisha
fasterq-dump --split-files ERS525616
touch ERS525616.done

## 01_download_gorilla_data_05.sh 

#!/bin/bash
#SBATCH --job-name=sra
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl5.out # output file
#SBATCH --error=dl5.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=4GB # Memory limit of 4GB

mkdir -p ind02_Tuck && cd ind02_Tuck
fasterq-dump --split-files ERS168204
touch ERS168204.done
cd ../
mkdir -p ind03_Turimaso && cd ind03_Turimaso
fasterq-dump --split-files ERS525618
touch ERS525618.done
cd ../
mkdir -p ind04_Umurimo && cd ind04_Umurimo
fasterq-dump --split-files ERS525617
touch ERS525617.done
cd ../
mkdir -p ind05_Imfura && cd ind05_Imfura
fasterq-dump --split-files ERS168207
touch ERS168207.done
cd ../
mkdir -p ind06_Kaboko && cd ind06_Kaboko
fasterq-dump --split-files ERS168410
touch ERS168410.done
cd ../
mkdir -p ind07_Zirikana && cd ind07_Zirikana
fasterq-dump --split-files ERS168174
touch ERS168174.done

## 01_download_gorilla_data_06.sh 

#!/bin/bash
#SBATCH --job-name=sra
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl6.out # output file
#SBATCH --error=dl6.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=4GB # Memory limit of 4GB

mkdir -p ind08_Dunia && cd ind08_Dunia
fasterq-dump --split-files ERS525621
touch ERS525621.done
cd ../
mkdir -p ind09_Itebero && cd ind09_Itebero
fasterq-dump --split-files ERS168205
touch ERS168205.done
cd ../
mkdir -p ind10_Pinga && cd ind10_Pinga
fasterq-dump --split-files ERS525620
touch ERS525620.done
cd ../
mkdir -p ind11_Serufuli && cd ind11_Serufuli
fasterq-dump --split-files ERS525622
touch ERS525622.done
cd ../
mkdir -p ind12_Tumani && cd ind12_Tumani
fasterq-dump --split-files ERS525619
touch ERS525619.done
cd ../
mkdir -p ind13_Ntabwoba && cd ind13_Ntabwoba
fasterq-dump --split-files ERS168206
touch ERS168206.done



mkdir -p /global/scratch/users/rdekayne/gorilla_census/data/genomes && cd /global/scratch/users/rdekayne/gorilla_census/data/genomes
#https://github.com/marbl/Primates?tab=readme-ov-file
#https://genomeark.s3.amazonaws.com/index.html?prefix=species/Gorilla_gorilla/mGorGor1/assembly_curated/
