#

#move data to scratch dir (infiniband) using globus
#to: /global/scratch/users/rdekayne

cd /global/scratch/users/rdekayne/
mkdir -p /global/scratch/users/rdekayne/envs/

module load anaconda3
conda create -p /global/scratch/users/rdekayne/envs/sra
source activate base
conda activate /global/scratch/users/rdekayne/envs/sra
conda install bioconda::sra-tools

#####################################
#####################################
#            DATA
#####################################
#####################################

mkdir -p /global/scratch/users/rdekayne/gorilla_census && cd /global/scratch/users/rdekayne/gorilla_census

mkdir -p /global/scratch/users/rdekayne/gorilla_census/data && cd /global/scratch/users/rdekayne/gorilla_census/data

## 01_download_gorilla_data_01.sh
#!/bin/bash
#SBATCH --job-name=sra
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl.out # output file
#SBATCH --error=dl.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=4GB # Memory limit of 4GB

mkdir -p ind14_Mkubwa && cd ind14_Mkubwa
fasterq-dump --split-files SRX243452
touch SRX243452.done
fasterq-dump --split-files SRX243453
touch SRX243453.done

## 01_download_gorilla_data_02.sh
#!/bin/bash
#SBATCH --job-name=sra2
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl2.out # output file
#SBATCH --error=dl2.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=4GB # Memory limit of 4GB

mkdir -p ind15_Kaisi && cd ind15_Kaisi
fasterq-dump --split-files SRX242685
touch SRX242685.done
fasterq-dump --split-files SRX242686
touch SRX242686.done
fasterq-dump --split-files SRX242687
touch SRX242687.done
fasterq-dump --split-files SRX242688
touch SRX242688.done

## 01_download_gorilla_data_03.sh
#!/bin/bash
#SBATCH --job-name=sra
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl3.out # output file
#SBATCH --error=dl3.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=4GB # Memory limit of 4GB

mkdir -p ind16_Victoria && cd ind16_Victoria
fasterq-dump --split-files SRX243528
touch SRX243528.done
fasterq-dump --split-files SRX243529
touch SRX243529.done
fasterq-dump --split-files SRX243530
touch SRX243530.done
fasterq-dump --split-files SRX243531
touch SRX243531.done
fasterq-dump --split-files SRX243532
touch SRX243532.done
fasterq-dump --split-files SRX243533
touch SRX243533.done

## 01_download_gorilla_data_04.sh
#!/bin/bash
#SBATCH --job-name=wget04
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl4.out # output file
#SBATCH --error=dl4.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind02_Tuck && cd ind02_Tuck
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223795/ERR223795_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223735/ERR223735_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223759/ERR223759_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223741/ERR223741_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223711/ERR223711_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223771/ERR223771_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223717/ERR223717_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223729/ERR223729_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223801/ERR223801_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223807/ERR223807_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223747/ERR223747_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223753/ERR223753_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223729/ERR223729_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223777/ERR223777_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223765/ERR223765_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223807/ERR223807_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR225/ERR225697/ERR225697_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223783/ERR223783_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223771/ERR223771_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223801/ERR223801_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223789/ERR223789_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223717/ERR223717_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223747/ERR223747_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223765/ERR223765_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223723/ERR223723_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223783/ERR223783_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223723/ERR223723_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223759/ERR223759_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223741/ERR223741_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223735/ERR223735_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR225/ERR225697/ERR225697_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223789/ERR223789_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223711/ERR223711_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223753/ERR223753_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223777/ERR223777_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223795/ERR223795_2.fastq.gz
touch ERS168204_others.done

#01_download_gorilla_data_05.sh
#!/bin/bash
#SBATCH --job-name=wget05
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl5.out # output file
#SBATCH --error=dl5.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind03_Turimaso && cd ind03_Turimaso
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668425/ERR668425_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668425/ERR668425_2.fastq.gz
touch ERR668425.done

## 01_download_gorilla_data_06.sh
#!/bin/bash
#SBATCH --job-name=wget06
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl6.out # output file
#SBATCH --error=dl6.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind05_Imfura && cd ind05_Imfura
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223776/ERR223776_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223806/ERR223806_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223746/ERR223746_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR225/ERR225696/ERR225696_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223740/ERR223740_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223782/ERR223782_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223710/ERR223710_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223734/ERR223734_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR225/ERR225696/ERR225696_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223746/ERR223746_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223752/ERR223752_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223800/ERR223800_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223764/ERR223764_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223770/ERR223770_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223776/ERR223776_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223722/ERR223722_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223794/ERR223794_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223728/ERR223728_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223758/ERR223758_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223716/ERR223716_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223740/ERR223740_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223788/ERR223788_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223716/ERR223716_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223758/ERR223758_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223764/ERR223764_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223722/ERR223722_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223794/ERR223794_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223728/ERR223728_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223770/ERR223770_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223710/ERR223710_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223800/ERR223800_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223752/ERR223752_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223806/ERR223806_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223788/ERR223788_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223734/ERR223734_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223782/ERR223782_1.fastq.gz
touch ERS168207_others.done

## 01_download_gorilla_data_07.sh
#!/bin/bash
#SBATCH --job-name=wet07
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl7.out # output file
#SBATCH --error=dl7.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind06_Kaboko && cd ind06_Kaboko
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223768/ERR223768_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223792/ERR223792_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223726/ERR223726_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223720/ERR223720_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223750/ERR223750_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223786/ERR223786_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223738/ERR223738_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223762/ERR223762_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223744/ERR223744_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR225/ERR225700/ERR225700_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223732/ERR223732_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223714/ERR223714_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223780/ERR223780_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223774/ERR223774_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223798/ERR223798_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223810/ERR223810_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223804/ERR223804_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223774/ERR223774_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223714/ERR223714_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223756/ERR223756_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223726/ERR223726_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223738/ERR223738_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223720/ERR223720_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223732/ERR223732_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223768/ERR223768_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223756/ERR223756_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223750/ERR223750_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223786/ERR223786_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223810/ERR223810_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223792/ERR223792_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223804/ERR223804_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223798/ERR223798_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223744/ERR223744_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223762/ERR223762_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223780/ERR223780_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR225/ERR225700/ERR225700_2.fastq.gz
touch ERS168410_others.done

## 01_download_gorilla_data_08.sh
#!/bin/bash
#SBATCH --job-name=wget08
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl8.out # output file
#SBATCH --error=dl8.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind07_Zirikana && cd ind07_Zirikana
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR225/ERR225695/ERR225695_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223787/ERR223787_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223745/ERR223745_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223727/ERR223727_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223793/ERR223793_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223733/ERR223733_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223751/ERR223751_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223775/ERR223775_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223769/ERR223769_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223805/ERR223805_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223751/ERR223751_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223745/ERR223745_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223793/ERR223793_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223799/ERR223799_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223709/ERR223709_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223757/ERR223757_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223739/ERR223739_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223715/ERR223715_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223763/ERR223763_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223733/ERR223733_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223739/ERR223739_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223721/ERR223721_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223805/ERR223805_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223775/ERR223775_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223781/ERR223781_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223799/ERR223799_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223757/ERR223757_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR225/ERR225695/ERR225695_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223781/ERR223781_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223787/ERR223787_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223727/ERR223727_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223769/ERR223769_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223715/ERR223715_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223763/ERR223763_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223721/ERR223721_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223709/ERR223709_1.fastq.gz
touch ERS168174_others.done

## 01_download_gorilla_data_09.sh
#!/bin/bash
#SBATCH --job-name=wget09
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl9.out # output file
#SBATCH --error=dl9.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind08_Dunia && cd ind08_Dunia
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668428/ERR668428_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668428/ERR668428_2.fastq.gz
touch ERS525621_others.done

## 01_download_gorilla_data_10.sh
#!/bin/bash
#SBATCH --job-name=wget10
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl10.out # output file
#SBATCH --error=dl10.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind09_Itebero && cd ind09_Itebero
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223784/ERR223784_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR225/ERR225698/ERR225698_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223742/ERR223742_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223754/ERR223754_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223718/ERR223718_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223808/ERR223808_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223778/ERR223778_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223796/ERR223796_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223736/ERR223736_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223760/ERR223760_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223790/ERR223790_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223778/ERR223778_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223766/ERR223766_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223808/ERR223808_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223748/ERR223748_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223784/ERR223784_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223802/ERR223802_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223712/ERR223712_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223760/ERR223760_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223742/ERR223742_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223724/ERR223724_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223790/ERR223790_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223730/ERR223730_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223766/ERR223766_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223754/ERR223754_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223748/ERR223748_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223796/ERR223796_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223772/ERR223772_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223802/ERR223802_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223724/ERR223724_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223730/ERR223730_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR225/ERR225698/ERR225698_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223772/ERR223772_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223718/ERR223718_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223736/ERR223736_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223712/ERR223712_1.fastq.gz
touch ERS168205_others.done

## 01_download_gorilla_data_11.sh
#!/bin/bash
#SBATCH --job-name=wget11
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl11.out # output file
#SBATCH --error=dl11.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind10_Pinga && cd ind10_Pinga
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668427/ERR668427_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668427/ERR668427_1.fastq.gz
touch ERS525620_others.done

## 01_download_gorilla_data_12.sh
#!/bin/bash
#SBATCH --job-name=wget12
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl12.out # output file
#SBATCH --error=dl12.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind13_Ntabwoba && cd ind13_Ntabwoba
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223809/ERR223809_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223779/ERR223779_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223761/ERR223761_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223737/ERR223737_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223791/ERR223791_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR225/ERR225699/ERR225699_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223713/ERR223713_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223743/ERR223743_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223773/ERR223773_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223779/ERR223779_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223803/ERR223803_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223749/ERR223749_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223767/ERR223767_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223719/ERR223719_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223713/ERR223713_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223731/ERR223731_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223737/ERR223737_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223785/ERR223785_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223755/ERR223755_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223725/ERR223725_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223797/ERR223797_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR225/ERR225699/ERR225699_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223719/ERR223719_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223803/ERR223803_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223761/ERR223761_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223731/ERR223731_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223785/ERR223785_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223773/ERR223773_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223755/ERR223755_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223767/ERR223767_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223809/ERR223809_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223743/ERR223743_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223749/ERR223749_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223725/ERR223725_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223797/ERR223797_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223791/ERR223791_1.fastq.gz
touch ERS168206_others.done

## 01_download_gorilla_data_13.sh
#!/bin/bash
#SBATCH --job-name=wget1
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl13.out # output file
#SBATCH --error=dl13.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind01_Maisha && cd ind01_Maisha

wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668423/ERR668423_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668423/ERR668423_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668423/ERR668423.fastq.gz
touch ind01.done

## 01_download_gorilla_data_14.sh
#!/bin/bash
#SBATCH --job-name=wget2
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl14.out # output file
#SBATCH --error=dl14.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind04_Umurimo && cd ind04_Umurimo

wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668424/ERR668424_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668424/ERR668424.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668424/ERR668424_2.fastq.gz
touch ind04.done

## 01_download_gorilla_data_15.sh
#!/bin/bash
#SBATCH --job-name=wget3
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl15.out # output file
#SBATCH --error=dl15.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind11_Serufuli && cd ind11_Serufuli
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668429/ERR668429_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668429/ERR668429.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668429/ERR668429_1.fastq.gz
touch ind11.done

## 01_download_gorilla_data_16.sh
#!/bin/bash
#SBATCH --job-name=wget4
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=dl16.out # output file
#SBATCH --error=dl16.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind12_Tumani && cd ind12_Tumani
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668426/ERR668426_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668426/ERR668426.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR668/ERR668426/ERR668426_1.fastq.gz
touch ind12.done

#!/bin/bash
#SBATCH --job-name=BONUS1
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=BONUS1.out # output file
#SBATCH --error=BONUS1.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

cd ind02_Tuck
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223807/ERR223807_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223747/ERR223747_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223753/ERR223753_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223729/ERR223729_2.fastq.gz

cd ../ind05_Imfura
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223752/ERR223752_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223800/ERR223800_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223764/ERR223764_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223770/ERR223770_1.fastq.gz


#!/bin/bash
#SBATCH --job-name=BONUS2
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=BONUS2.out # output file
#SBATCH --error=BONUS2.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

cd ind06_Kaboko
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223762/ERR223762_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223744/ERR223744_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR225/ERR225700/ERR225700_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223732/ERR223732_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223714/ERR223714_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223780/ERR223780_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223774/ERR223774_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223798/ERR223798_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223810/ERR223810_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223804/ERR223804_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223774/ERR223774_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223714/ERR223714_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223756/ERR223756_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223726/ERR223726_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223738/ERR223738_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223720/ERR223720_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223732/ERR223732_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223768/ERR223768_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223756/ERR223756_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223750/ERR223750_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223786/ERR223786_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223810/ERR223810_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223792/ERR223792_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223804/ERR223804_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223798/ERR223798_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223744/ERR223744_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223762/ERR223762_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223780/ERR223780_2.fastq.gz

#!/bin/bash
#SBATCH --job-name=BONUS3
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=BONUS3.out # output file
#SBATCH --error=BONUS3.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

cd ind07_Zirikana
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223775/ERR223775_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223769/ERR223769_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223805/ERR223805_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223751/ERR223751_1.fastq.gz

cd ../ind13_Ntabwoba
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223743/ERR223743_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223773/ERR223773_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223779/ERR223779_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223803/ERR223803_1.fastq.gz


#!/bin/bash
#SBATCH --job-name=BONUS4
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=BONUS4.out # output file
#SBATCH --error=BONUS4.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

cd ind09_Itebero
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223796/ERR223796_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223736/ERR223736_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223760/ERR223760_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223790/ERR223790_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223778/ERR223778_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223766/ERR223766_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223808/ERR223808_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223748/ERR223748_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223784/ERR223784_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223802/ERR223802_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223712/ERR223712_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223760/ERR223760_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223742/ERR223742_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223724/ERR223724_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223790/ERR223790_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223730/ERR223730_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223766/ERR223766_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223754/ERR223754_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223748/ERR223748_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223796/ERR223796_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR223/ERR223772/ERR223772_2.fastq.gz

##deleted unpaired files:
rm ind01_Maisha/ERR668423.fastq.gz
rm ind04_Umurimo/ERR668424.fastq.gz
rm ind12_Tumani/ERR668426.fastq.gz
rm ind15_Kaisi/SRR747657.fastq.gz
rm ind15_Kaisi/SRR747658.fastq.gz

#####Genomes
mkdir -p /global/scratch/users/rdekayne/gorilla_census/data/genomes && cd /global/scratch/users/rdekayne/gorilla_census/data/genomes
#https://github.com/marbl/Primates?tab=readme-ov-file
#https://genomeark.s3.amazonaws.com/index.html?prefix=species/Gorilla_gorilla/mGorGor1/assembly_curated/

####################extra mountain gorillas


## 01_download_gorilla_data_17.sh

#!/bin/bash
#SBATCH --job-name=wget2_17
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=wget2_17.out # output file
#SBATCH --error=wget2_17.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind17_Bwiruka && cd ind17_Bwiruka
#17
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR230/005/ERR2300765/ERR2300765_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR230/005/ERR2300765/ERR2300765_1.fastq.gz
touch ind17.done

##run
sbatch 01_download_gorilla_data_17.sh

## 01_download_gorilla_data_18.sh

#!/bin/bash
#SBATCH --job-name=wget2_18
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=wget2_18.out # output file
#SBATCH --error=wget2_18.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind18_Kahungye && cd ind18_Kahungye
#18
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR230/002/ERR2300762/ERR2300762_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR230/002/ERR2300762/ERR2300762_2.fastq.gz
touch ind18.done

##run
sbatch 01_download_gorilla_data_18.sh

## 01_download_gorilla_data_19.sh

#!/bin/bash
#SBATCH --job-name=wget2_19
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=wget2_19.out # output file
#SBATCH --error=wget2_19.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind19_Katungi && cd ind19_Katungi
#19
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR230/003/ERR2300763/ERR2300763_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR230/003/ERR2300763/ERR2300763_2.fastq.gz
touch ind19.done

##run
sbatch 01_download_gorilla_data_19.sh

## 01_download_gorilla_data_20.sh

#!/bin/bash
#SBATCH --job-name=wget2_20
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=wget2_20.out # output file
#SBATCH --error=wget2_20.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind20_Nyamunwa && cd ind20_Nyamunwa
#20
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR230/004/ERR2300764/ERR2300764_1.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR230/004/ERR2300764/ERR2300764_2.fastq.gz
touch ind20.done

##run
sbatch 01_download_gorilla_data_20.sh

## 01_download_gorilla_data_21.sh

#!/bin/bash
#SBATCH --job-name=wget2_21
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=savio_lowprio
#SBATCH --output=wget2_21.out # output file
#SBATCH --error=wget2_21.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=20GB # Memory limit of 4GB

mkdir -p ind21_Semehe && cd ind21_Semehe
#21
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR230/006/ERR2300766/ERR2300766_2.fastq.gz
wget -nc ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR230/006/ERR2300766/ERR2300766_1.fastq.gz
touch ind21.done

##run
sbatch 01_download_gorilla_data_21.sh


#####################################
#####################################
#            O1_mapping
#####################################
#####################################

mkdir -p /global/scratch/users/rdekayne/gorilla_census/01_mapping && cd /global/scratch/users/rdekayne/gorilla_census/01_mapping
mkdir /global/scratch/users/rdekayne/gorilla_census/01_mapping/fastp_fastqs

conda create -p /global/scratch/users/rdekayne/envs/fastp
conda activate /global/scratch/users/rdekayne/envs/fastp
conda install bioconda::fastp

## 01.1_mapping_qc_p1.sh 

#!/bin/bash
#SBATCH --job-name=fastp_test
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --qos=savio_lowprio
#SBATCH --output=fastq1.%j.out # output file
#SBATCH --error=fastq1.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=16 # 2 CPUs per job

cd /global/scratch/users/rdekayne/gorilla_census/01_mapping

ind=${SLURM_ARRAY_TASK_ID}
#cat /global/scratch/users/rdekayne/gorilla_census/01_mapping/sample_list.txt | sed -n ${ind}p > ${SLURM_ARRAY_TASK_ID}.sample_list.txt
cat /global/scratch/users/rdekayne/gorilla_census/01_mapping/sample_list2.txt | sed -n ${ind}p > ${SLURM_ARRAY_TASK_ID}.sample_list.txt

VAR1=$(cut -d ' ' -f1 ${SLURM_ARRAY_TASK_ID}.sample_list.txt)
VAR2=$(cut -d ' ' -f2 ${SLURM_ARRAY_TASK_ID}.sample_list.txt)

echo $VAR1
echo $VAR2

fastp -i /global/scratch/users/rdekayne/gorilla_census/data/${VAR1}/${VAR2}_1.fastq.gz -I /global/scratch/users/rdekayne/gorilla_census/data/${VAR1}/${VAR2}_2.fastq.gz --thread 16 -o /global/scratch/users/rdekayne/gorilla_census/01_mapping/fastp_fastqs/${VAR1}_${VAR2}_1.out.fastq.gz -O /global/scratch/users/rdekayne/gorilla_census/01_mapping/fastp_fastqs/${VAR1}_${VAR2}_2.out.fastq.gz -j ${VAR1}_${VAR2}.json -h ${VAR1}_${VAR2}.html && touch ${VAR1}_${VAR2}.done

rm ${SLURM_ARRAY_TASK_ID}.sample_list.txt

##run
sbatch --array=1-131 01.1_mapping_qc_p1.sh

sbatch --array=1-5 01.1_mapping_qc_p1.sh

#########################################################################################
##multiqc 
eval "$(/global/scratch/users/scott_ferguson/modules/miniconda3/bin/conda shell.zsh hook)"
mamba activate multiqc

mkdir -p /global/scratch/users/rdekayne/gorilla_census/01_mapping/html_output && cd /global/scratch/users/rdekayne/gorilla_census/01_mapping/html_output
mv *.html /global/scratch/users/rdekayne/gorilla_census/01_mapping/html_output

mkdir -p /global/scratch/users/rdekayne/gorilla_census/01_mapping/json_output && cd /global/scratch/users/rdekayne/gorilla_census/01_mapping/json_output
mv *.json /global/scratch/users/rdekayne/gorilla_census/01_mapping/json_output

cd /global/scratch/users/rdekayne/gorilla_census/01_mapping/json_output
multiqc --fn_as_s_name . 
#########################################################################################

#install bwa-mem2 + samtools + sambamba
conda create -p /global/scratch/users/rdekayne/envs/mapping
conda activate /global/scratch/users/rdekayne/envs/mapping
conda install bioconda::samtools
conda install bioconda::sambamba
conda install bioconda::bwa-mem2
conda install bioconda::bcftools
conda install bioconda::picard
conda install bioconda::mosdepth


#index assembly
# 01.2_mapping_index_p1.sh 
#!/bin/bash
#SBATCH --job-name=index
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --qos=savio_lowprio
#SBATCH --output=index.out # output file
#SBATCH --error=index.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer

cd /global/scratch/users/rdekayne/gorilla_census/data/genomes
bwa-mem2 index mGorGor1.pri.cur.20231122.fasta

##run


mkdir /global/scratch/users/rdekayne/gorilla_census/01_mapping/raw_bams
# 01.3_mapping_map_p1.sh 

#!/bin/bash
#SBATCH --job-name=bwa_map
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=genomicdata_htc4_normal
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=20
#SBATCH --mem=100G
#SBATCH --time=24:00:00
#SBATCH --output=bwa_map.%j.out
#SBATCH --error=bwa_map.%j.err
#SBATCH -w n0067.savio4

cd /global/scratch/users/rdekayne/gorilla_census/01_mapping

ind=${SLURM_ARRAY_TASK_ID}
#cat /global/scratch/users/rdekayne/gorilla_census/01_mapping/sample_list.txt | sed -n ${ind}p > ${SLURM_ARRAY_TASK_ID}.sample_list.txt
cat /global/scratch/users/rdekayne/gorilla_census/01_mapping/sample_list2.txt | sed -n ${ind}p > ${SLURM_ARRAY_TASK_ID}.sample_list.txt

VAR1=$(cut -d ' ' -f1 ${SLURM_ARRAY_TASK_ID}.sample_list.txt)
VAR2=$(cut -d ' ' -f2 ${SLURM_ARRAY_TASK_ID}.sample_list.txt)

# call bwa
bwa-mem2 mem -t 24 /global/scratch/users/rdekayne/gorilla_census/data/genomes/mGorGor1.pri.cur.20231122.fasta /global/scratch/users/rdekayne/gorilla_census/01_mapping/fastp_fastqs/${VAR1}_${VAR2}_1.out.fastq.gz /global/scratch/users/rdekayne/gorilla_census/01_mapping/fastp_fastqs/${VAR1}_${VAR2}_2.out.fastq.gz | samtools sort -@24 -o /global/scratch/users/rdekayne/gorilla_census/01_mapping/raw_bams/${VAR1}_${VAR2}.raw.bam && touch ${VAR1}_${VAR2}.mapping.done

##run
sbatch --array=1-131 01.3_mapping_map_p1.sh 
sbatch --array=1-5%1 01.3_mapping_map_p1.sh 

module load java/22.0.1

# 01.4_mapping_process_p1.sh 

#!/bin/bash
#SBATCH --job-name=bwa_proc
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=genomicdata_htc4_normal
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=24
#SBATCH --mem=100G
#SBATCH --time=24:00:00
#SBATCH --output=bwa_proc.%j.out
#SBATCH --error=bwa_proc.%j.err

cd /global/scratch/users/rdekayne/gorilla_census/01_mapping

ind=${SLURM_ARRAY_TASK_ID}
#cat /global/scratch/users/rdekayne/gorilla_census/01_mapping/sample_list.txt | sed -n ${ind}p > ${SLURM_ARRAY_TASK_ID}.sample_list.txt
cat /global/scratch/users/rdekayne/gorilla_census/01_mapping/sample_list2.txt | sed -n ${ind}p > ${SLURM_ARRAY_TASK_ID}.sample_list.txt

VAR1=$(cut -d ' ' -f1 ${SLURM_ARRAY_TASK_ID}.sample_list.txt)
VAR2=$(cut -d ' ' -f2 ${SLURM_ARRAY_TASK_ID}.sample_list.txt)

#PART1
picard FixMateInformation I=/global/scratch/users/rdekayne/gorilla_census/01_mapping/raw_bams/${VAR1}_${VAR2}.raw.bam VALIDATION_STRINGENCY=LENIENT OUTPUT=/global/scratch/users/rdekayne/gorilla_census/01_mapping/raw_bams/tmp_bam/${VAR1}_${VAR2}.raw.bam

sambamba sort/global/scratch/users/rdekayne/gorilla_census/01_mapping/raw_bams/tmp_bam/${VAR1}_${VAR2}.raw.bam -o /global/scratch/users/rdekayne/gorilla_census/01_mapping/raw_bams/tmp_bam/${VAR1}_${VAR2}.sorted.raw.bam -t 24 -m 50GB

picard MarkDuplicates INPUT=/global/scratch/users/rdekayne/gorilla_census/01_mapping/raw_bams/tmp_bam/${VAR1}_${VAR2}.sorted.raw.bam OUTPUT=/global/scratch/users/rdekayne/gorilla_census/01_mapping/processed_bams/${VAR1}_${VAR2}.sorted.dup.bam METRICS_FILE=/global/scratch/users/rdekayne/gorilla_census/01_mapping/processed_bams/${VAR1}_${VAR2}.sorted.dup.txt VALIDATION_STRINGENCY=LENIENT MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=1024

sambamba index /global/scratch/users/rdekayne/gorilla_census/01_mapping/processed_bams/${VAR1}_${VAR2}.sorted.dup.bam 

#now remove all intermediate files
rm /global/scratch/users/rdekayne/gorilla_census/01_mapping/raw_bams/tmp_bam/${VAR1}_${VAR2}*
rm ${SLURM_ARRAY_TASK_ID}.sample_list.txt

touch ${VAR1}_${VAR2}.processing.done

##submit
sbatch --array=1-131%8 01.4_mapping_process_p1.sh 


mkdir -p /global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams

# 01.5_mapping_merge_p1.sh 

#!/bin/bash
#SBATCH --job-name=bwa_merg
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3_bigmem
#SBATCH --output=bwa_merg.%j.out # output file
#SBATCH --error=bwa_merg.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=24 # 2 CPUs per job

cd /global/scratch/users/rdekayne/gorilla_census/01_mapping

ind=${SLURM_ARRAY_TASK_ID}
indiv_name=$(cat /global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/merge.list | sed -n ${ind}p)

samtools merge -o /global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/${indiv_name}_1file.bam /global/scratch/users/rdekayne/gorilla_census/01_mapping/processed_bams/${indiv_name}*.bam

sambamba index /global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/${indiv_name}_1file.bam

touch ${indiv_name}.mege.done

##submit
sbatch --array=1-9 01.5_mapping_merge_p1.sh

cp ind01_Maisha_ERR668423.sorted.dup.bam ../indiv_bams/ind01_Maisha_1file.bam
cp ind01_Maisha_ERR668423.sorted.dup.bam.bai ../indiv_bams/ind01_Maisha_1file.bam.bai
cp ind01_Maisha_ERR668423.sorted.dup.txt ../indiv_bams/ind01_Maisha_1file.txt
cp ind03_Turimaso_ERR668425.sorted.dup.bam ../indiv_bams/ind03_Turimaso_1file.bam
cp ind03_Turimaso_ERR668425.sorted.dup.bam.bai ../indiv_bams/ind03_Turimaso_1file.bam.bai
cp ind03_Turimaso_ERR668425.sorted.dup.txt ../indiv_bams/ind03_Turimaso_1file.txt
cp ind04_Umurimo_ERR668424.sorted.dup.bam ../indiv_bams/ind04_Umurimo_1file.bam
cp ind04_Umurimo_ERR668424.sorted.dup.bam.bai ../indiv_bams/ind04_Umurimo_1file.bam.bai
cp ind04_Umurimo_ERR668424.sorted.dup.txt ../indiv_bams/ind04_Umurimo_1file.txt
cp ind08_Dunia_ERR668428.sorted.dup.bam ../indiv_bams/ind08_Dunia_1file.bam
cp ind08_Dunia_ERR668428.sorted.dup.bam.bai ../indiv_bams/ind08_Dunia_1file.bam.bai
cp ind08_Dunia_ERR668428.sorted.dup.txt ../indiv_bams/ind08_Dunia_1file.txt
cp ind10_Pinga_ERR668427.sorted.dup.bam ../indiv_bams/ind10_Pinga_1file.bam
cp ind10_Pinga_ERR668427.sorted.dup.bam.bai ../indiv_bams/ind10_Pinga_1file.bam.bai
cp ind10_Pinga_ERR668427.sorted.dup.txt ../indiv_bams/ind10_Pinga_1file.txt
cp ind11_Serufuli_ERR668429.sorted.dup.bam ../indiv_bams/ind11_Serufuli_1file.bam
cp ind11_Serufuli_ERR668429.sorted.dup.bam.bai ../indiv_bams/ind11_Serufuli_1file.bam.bai
cp ind11_Serufuli_ERR668429.sorted.dup.txt ../indiv_bams/ind11_Serufuli_1file.txt
cp ind12_Tumani_ERR668426.sorted.dup.bam ../indiv_bams/ind12_Tumani_1file.bam
cp ind12_Tumani_ERR668426.sorted.dup.bam.bai ../indiv_bams/ind12_Tumani_1file.bam.bai
cp ind12_Tumani_ERR668426.sorted.dup.txt ../indiv_bams/ind12_Tumani_1file.txt

cp ind17_Bwiruka_ERR2300765.sorted.dup.bam ../indiv_bams/ind17_Bwiruka_1file.bam
cp ind17_Bwiruka_ERR2300765.sorted.dup.bam.bai ../indiv_bams/ind17_Bwiruka_1file.bam.bai
cp ind17_Bwiruka_ERR2300765.sorted.dup.txt ../indiv_bams/ind17_Bwiruka_1file.txt
cp ind19_Katungi_ERR2300763.sorted.dup.bam ../indiv_bams/ind19_Katungi_1file.bam
cp ind19_Katungi_ERR2300763.sorted.dup.bam.bai ../indiv_bams/ind19_Katungi_1file.bam.bai
cp ind19_Katungi_ERR2300763.sorted.dup.txt ../indiv_bams/ind19_Katungi_1file.txt
cp ind20_Nyamunwa_ERR2300764.sorted.dup.bam ../indiv_bams/ind20_Nyamunwa_1file.bam
cp ind20_Nyamunwa_ERR2300764.sorted.dup.bam.bai ../indiv_bams/ind20_Nyamunwa_1file.bam.bai
cp ind20_Nyamunwa_ERR2300764.sorted.dup.txt ../indiv_bams/ind20_Nyamunwa_1file.txt
cp ind21_Semehe_ERR2300766.sorted.dup.bam ../indiv_bams/ind21_Semehe_1file.bam
cp ind21_Semehe_ERR2300766.sorted.dup.bam.bai ../indiv_bams/ind21_Semehe_1file.bam.bai
cp ind21_Semehe_ERR2300766.sorted.dup.txt ../indiv_bams/ind21_Semehe_1file.txt

cd /global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams && ls *.bam > bam_1file.list

#bam_2file.list
ind17_Bwiruka_1file.bam
ind19_Katungi_1file.bam
ind20_Nyamunwa_1file.bam
ind21_Semehe_1file.bam

# 01.6_mapping_mosdepth_p1.sh 

#!/bin/bash
#SBATCH --job-name=mos
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=mos.%j.out # output file
#SBATCH --error=mos.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=24 # 2 CPUs per job

cd /global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams

ind=${SLURM_ARRAY_TASK_ID}
indiv_name=$(cat bam_1file.list | sed -n ${ind}p)

mosdepth -n ${indiv_name} /global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/${indiv_name} && touch ${indiv_name}.mosdepth.done 
samtools flagstat ${indiv_name} && touch ${indiv_name}.flagstst.done

##run 
sbatch --array=1-16 01.6_mapping_mosdepth_p1.sh 

# 01.6_mapping_mosdepth_p2.sh 

#!/bin/bash
#SBATCH --job-name=mos
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --output=mos.%j.out # output file
#SBATCH --error=mos.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=24 # 2 CPUs per job

cd /global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams

ind=${SLURM_ARRAY_TASK_ID}
indiv_name=$(cat bam_2file.list | sed -n ${ind}p)

mosdepth -n ${indiv_name} /global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/${indiv_name} && touch ${indiv_name}.mosdepth.done 
samtools flagstat ${indiv_name} && touch ${indiv_name}.flagstst.done

##run 
sbatch --array=1-4 01.6_mapping_mosdepth_p2.sh 

#####################################
#####################################
#            02_genotyping 
#####################################
#####################################
conda create -p /global/scratch/users/rdekayne/envs/geno
conda activate /global/scratch/users/rdekayne/envs/geno
conda install bioconda::bcftools

#make genotyping directory
mkdir -p /global/scratch/users/rdekayne/gorilla_census/02_genotyping/ && cd /global/scratch/users/rdekayne/gorilla_census/02_genotyping/
#and make a directory for the done-files
mkdir -p /global/scratch/users/rdekayne/gorilla_census/02_genotyping/done_files
#and make directory for raw vcf files
mkdir -p /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_vcfs

#want to get scaffolds in ascending order:
cut -f1 /global/scratch/users/rdekayne/gorilla_census/data/genomes/mGorGor1.pri.cur.20231122.fasta.gz.fai > /global/scratch/users/rdekayne/gorilla_census/02_genotyping/scaf.list

#make bam list too
ls /global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/*.bam > bam.list

#get rid of 08-16 for low coverage and being eastern lowland
cat bam.list | grep -v 'ind08' |  grep -v 'ind09' |  grep -v 'ind10' |  grep -v 'ind11' |  grep -v 'ind12' | grep -v 'ind13' |  grep -v 'ind14' |  grep -v 'ind15' |  grep -v 'ind16' > 11_bam_list.txt

## 02.1_genotyping.sh

#!/bin/bash
#SBATCH --job-name=geno
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=genomicdata_htc4_normal
#SBATCH --output=geno.%j.out # output file
#SBATCH --error=geno.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=4 # 2 CPUs per job
#SBATCH -w n0032.savio4

numb=${SLURM_ARRAY_TASK_ID}
scaf_name=$(cat w | sed -n ${numb}p)

bcftools mpileup -O u -d 250 --skip-indels -f /global/scratch/users/rdekayne/gorilla_census/data/genomes/mGorGor1.pri.cur.20231122.fasta --annotate FORMAT/DP --bam-list /global/scratch/users/rdekayne/gorilla_census/02_genotyping/11_bam_list.txt -r "${scaf_name}" | bcftools call -m -f GQ -O v | bgzip > /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_vcfs/"${scaf_name}".raw.vcf.gz && touch /global/scratch/users/rdekayne/gorilla_census/02_genotyping/done_files/"${scaf_name}".raw.vcf.gz.done

#run
sbatch --array=1-23%1 02.1_genotyping.sh

##sex chromosome genotyping
#ploidy.txt
/global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/ind01_Maisha_1file.bam  F
/global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/ind02_Tuck_1file.bam  F
/global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/ind03_Turimaso_1file.bam  F
/global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/ind04_Umurimo_1file.bam  F
/global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/ind05_Imfura_1file.bam  M
/global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/ind06_Kaboko_1file.bam  M
/global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/ind07_Zirikana_1file.bam  M
/global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/ind17_Bwiruka_1file.bam  F
/global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/ind19_Katungi_1file.bam  F
/global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/ind20_Nyamunwa_1file.bam  F
/global/scratch/users/rdekayne/gorilla_census/01_mapping/indiv_bams/ind21_Semehe_1file.bam  F

cat ploidy.txt | sed 's/ F/ 2/g' | sed 's/ M/ 1/g' > X_ploidy.txt
cat ploidy.txt | sed 's/ F/0/g' | sed 's/ M/ 1/g' > Y_ploidy.txt

## 02.2_genotyping.sh

#!/bin/bash
#SBATCH --job-name=geno_s
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=genomicdata_htc4_normal
#SBATCH --output=geno_s.%j.out # output file
#SBATCH --error=geno_s.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=4 # 2 CPUs per job
#SBATCH -w n0032.savio4

bcftools mpileup -O u -d 250 --skip-indels -f /global/scratch/users/rdekayne/gorilla_census/data/genomes/mGorGor1.pri.cur.20231122.fasta --annotate FORMAT/DP --bam-list /global/scratch/users/rdekayne/gorilla_census/02_genotyping/11_bam_list.txt -r chrX_mat_hsaX | bcftools call -m --samples-file X_ploidy.txt -f GQ -O v | bgzip > /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_vcfs/chrX_mat_hsaX.raw.vcf.gz && touch /global/scratch/users/rdekayne/gorilla_census/02_genotyping/done_files/chrX_mat_hsaX.raw.vcf.gz.done

bcftools mpileup -O u -d 250 --skip-indels -f /global/scratch/users/rdekayne/gorilla_census/data/genomes/mGorGor1.pri.cur.20231122.fasta --annotate FORMAT/DP --bam-list /global/scratch/users/rdekayne/gorilla_census/02_genotyping/11_bam_list.txt -r chrY_pat_hsaY | bcftools call -m --samples-file Y_ploidy.txt -f GQ -O v | bgzip > /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_vcfs/chrY_pat_hsaY.raw.vcf.gz && touch /global/scratch/users/rdekayne/gorilla_census/02_genotyping/done_files/chrY_pat_hsaY.raw.vcf.gz.done

#run
sbatch 02.2_genotyping.sh

bcftools query -l raw_vcfs/chr10_mat_hsa12.raw.vcf.gz  > old_names.txt
sed 's/\/global\/scratch\/users\/rdekayne\/gorilla_census\/01_mapping\/indiv_bams\///g' old_names.txt > new_names.txt
sed -i 's/_1file.bam//g' new_names.txt

paste old_names.txt new_names.txt > reheader.txt

mkdir -p /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_reheader_vcfs

## 02.3_genotyping_rehead.sh

#!/bin/bash
#SBATCH --job-name=rehead
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=rehead.%j.out # output file
#SBATCH --error=rehead.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=4 # 2 CPUs per job

numb=${SLURM_ARRAY_TASK_ID}
scaf_name=$(cat /global/scratch/users/rdekayne/gorilla_census/02_genotyping/scaf.list | sed -n ${numb}p)

bcftools reheader --samples /global/scratch/users/rdekayne/gorilla_census/02_genotyping/reheader.txt -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_reheader_vcfs/"${scaf_name}".reheader.raw.vcf.gz /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_vcfs/"${scaf_name}".raw.vcf.gz && touch /global/scratch/users/rdekayne/gorilla_census/02_genotyping/done_files/"${scaf_name}".rehead.done

#run
sbatch --array=1-25 02.3_genotyping_rehead.sh

## 02.4_vcf_index.sh

#!/bin/bash
#SBATCH --job-name=index
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=index.%j.out # output file
#SBATCH --error=index.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=4 # 2 CPUs per job

numb=${SLURM_ARRAY_TASK_ID}
scaf_name=$(cat /global/scratch/users/rdekayne/gorilla_census/02_genotyping/scaf.list | sed -n ${numb}p)

bcftools index /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_reheader_vcfs/"${scaf_name}".reheader.raw.vcf.gz

##run
sbatch --array=1-25 02.4_vcf_index.sh

mkdir -p /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_vcfs

#make autosome scaffold list
cat /global/scratch/users/rdekayne/gorilla_census/02_genotyping/scaf.list | grep -v "chrX" | grep -v "chrY" > autosomes_scaffold_list.txt

## 02.5_vcf_filt2.sh

#!/bin/bash
#SBATCH --job-name=filt
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=genomicdata_htc4_normal
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=50G
#SBATCH --time=24:00:00
#SBATCH --output=filt.%j.out
#SBATCH --error=filt.%j.err

numb=${SLURM_ARRAY_TASK_ID}
scaf_name=$(cat /global/scratch/users/rdekayne/gorilla_census/02_genotyping/autosomes_scaffold_list.txt | sed -n ${numb}p)

mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_"${scaf_name}"

bcftools filter -Ou /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_reheader_vcfs/"${scaf_name}".reheader.raw.vcf.gz -e 'FORMAT/DP < 7 | FORMAT/GQ < 30' --set-GTs . -O u | bcftools filter -e 'AN < 2' | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_temp_"${scaf_name}" -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_chr_vcfs2/"${scaf_name}".reheader_filt_mindepth7_minqual30.vcf.gz && touch /global/scratch/users/rdekayne/gorilla_census/02_genotyping/done_files/"${scaf_name}"_filt2.done

##run
sbatch --array=1-23 02.5_vcf_filt2.sh

cd /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_chr_vcfs2/ && ls *.vcf.gz > autosomes_filt2_to_merge.txt

## 02.6_vcf_auto_concat_filt.sh

#!/bin/bash
#SBATCH --job-name=concat
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=genomicdata_htc4_normal
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=20
#SBATCH --mem=100G
#SBATCH --time=24:00:00
#SBATCH --output=concat.%j.out
#SBATCH --error=concat.%j.err

mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_concat_merge
bcftools concat -Ou -f /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_chr_vcfs2/autosomes_filt2_to_merge.txt | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_concat_merge -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs2/autosomes_output_filt_mindepth7_minqual30.vcf.gz

mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_concat_merge2
#max 0 missing filter for SNPs
bcftools filter -Oz /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs2/autosomes_output_filt_mindepth7_minqual30.vcf.gz -e 'INFO/MAC < 1 | AN < 28' | bcftools view -m2 -M2 -v snps | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_concat_merge2 -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs2/autosomes_output_filt_mindepth7_minqual30_AN313.vcf.gz

touch concat_auto.done

## 02.7_vcf_sex_filt.sh

#!/bin/bash
#SBATCH --job-name=filt
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=genomicdata_htc4_normal
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=50G
#SBATCH --time=24:00:00
#SBATCH --output=filt.%j.out
#SBATCH --error=filt.%j.err

mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filtX
bcftools filter -Ou /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_reheader_vcfs/chrX_mat_hsaX.reheader.raw.vcf.gz -e 'FORMAT/DP < 7 | FORMAT/GQ < 30' --set-GTs . -O u | bcftools filter -e 'AN < 2' | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filtX -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_chr_vcfs2/chrX_mat_hsaX.reheader_filt_mindepth7_minqual30.vcf.gz && touch /global/scratch/users/rdekayne/gorilla_census/02_genotyping/done_files/chrX_mat_hsaX_filt2.done

mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filtX2
bcftools filter -Oz /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_chr_vcfs2/chrX_mat_hsaX.reheader_filt_mindepth7_minqual30.vcf.gz -e 'INFO/MAC < 1 | AN < 22' | bcftools view -m2 -M2 -v snps | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filtX2 -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs2/chrX_mat_hsaX_output_filt_mindepth7_minqual30_AN22.vcf.gz


mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filtY
bcftools filter -Ou /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_reheader_vcfs/chrY_pat_hsaY.reheader.raw.vcf.gz -e 'FORMAT/DP < 7 | FORMAT/GQ < 30' --set-GTs . -O u | bcftools filter -e 'AN < 2' | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filtY -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_chr_vcfs2/chrY_pat_hsaY.reheader_filt_mindepth7_minqual30.vcf.gz && touch /global/scratch/users/rdekayne/gorilla_census/02_genotyping/done_files/chrY_pat_hsaY_filt2.done

mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filtY2
bcftools filter -Oz /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_chr_vcfs2/chrY_pat_hsaY.reheader_filt_mindepth7_minqual30.vcf.gz -e 'INFO/MAC < 1 | AN < 6' | bcftools view -m2 -M2 -v snps | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filtY2 -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs2/chrY_pat_hsaY_output_filt_mindepth7_minqual30_AN06.vcf.gz

##run
sbatch 02.7_vcf_sex_filt.sh



## 02.7_vcf_concat_filt.sh

#!/bin/bash
#SBATCH --job-name=conc
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=conc.%j.out # output file
#SBATCH --error=conc.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=4 # 2 CPUs per job

mkdir -p /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs
mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat
mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filt

ls /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_vcfs/*AN28.vcf.gz > autosomes_to_merge.txt

bcftools concat -Ou -f autosomes_to_merge.txt | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_mindepth7_minqual30_m2M2_AN28.vcf.gz

ls /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_mindepth7_minqual30_m2M2_AN28.vcf.gz > auto_sex_vcf_merge.txt
ls /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_vcfs/chrX_mat_hsaX_filt_mindepth7_minqual30_m2M2_AN22.vcf.gz >> auto_sex_vcf_merge.txt
ls /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_vcfs/chrY_pat_hsaY_mindepth7_minqual30_m2M2_AN6.vcf.gz >> auto_sex_vcf_merge.txt

mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concatall
bcftools concat -Ou -f auto_sex_vcf_merge.txt | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concatall -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_X_Y_mindepth7_minqual30_mac1_AN28_AN22_AN6.vcf.gz

bcftools view -H /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_mindepth7_minqual30_m2M2_AN28.vcf.gz | wc -l
bcftools view -H /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_X_Y_mindepth7_minqual30_mac1_AN28_AN22_AN6.vcf.gz | wc -l

touch genotype_stats.done

##run
sbatch 02.7_vcf_concat_filt.sh

## 3764518
## 3823659

#change chromosome names:
chr_name_conversion.txt

chr1_pat_hsa1   chr1pathsa1
chr2_pat_hsa3   chr2pathsa3
chr3_pat_hsa4   chr3pathsa4
chr4_pat_hsa17x5    chr4pathsa17x5
chr5_mat_hsa6   chr5mathsa6
chr6_mat_hsa7   chr6mathsa7
chr7_pat_hsa8   chr7pathsa8
chr8_pat_hsa10  chr8pathsa10
chr9_pat_hsa11  chr9pathsa11
chr10_mat_hsa12 chr10mathsa12
chr11_mat_hsa2b chr11mathsa2b
chr12_pat_hsa2a chr12pathsa2a
chr13_pat_hsa9  chr13pathsa9
chr14_pat_hsa13 chr14pathsa13
chr15_pat_hsa14 chr15pathsa14
chr16_pat_hsa15 chr16pathsa15
chr17_mat_hsa18 chr17mathsa18
chr18_pat_hsa16 chr18pathsa16
chr19_pat_hsa5x17   chr19pathsa5x17
chr20_mat_hsa19 chr20mathsa19
chr21_pat_hsa20 chr21pathsa20
chr22_mat_hsa21 chr22mathsa21
chr23_mat_hsa22 chr23mathsa22

## 02.8_vcf_mpcrselect_prep.sh

#!/bin/bash
#SBATCH --job-name=prep
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=prep.%j.out # output file
#SBATCH --error=prep.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=4 # 2 CPUs per job

bcftools annotate --rename-chrs chr_name_conversion.txt /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_mindepth7_minqual30_m2M2_AN28.vcf.gz | bcftools sort -Oz -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_mindepth7_minqual30_m2M2_AN28_chrrename.vcf.gz

bcftools filter -Ou /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_mindepth7_minqual30_m2M2_AN28_chrrename.vcf.gz -e 'FORMAT/DP < 7 | (FORMAT/GQ) < 30' | bcftools sort -Oz -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_mindepth7_minqual30_m2M2_AN28_chrrename_refiltGQ30.vcf.gz

## run
sbatch 02.8_vcf_mpcrselect_prep.sh

bcftools view -H 14gorilla_autosomes_mindepth7_minqual30_m2M2_AN28_chrrename_refiltGQ30.vcf.gz | wc -l
## 3764518

#####################################
#####################################
#            03_PCA
#####################################
#####################################
conda create -p /global/scratch/users/rdekayne/envs/pca
conda activate /global/scratch/users/rdekayne/envs/pca
conda install bioconda::plink

mkdir -p /global/scratch/users/rdekayne/gorilla_census/03_PCA && cd /global/scratch/users/rdekayne/gorilla_census/03_PCA

# 03.1_PCA_unfilt.sh

#!/bin/bash
#SBATCH --job-name=pca
#SBATCH --time=0-4:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=pca.%j.out # output file
#SBATCH --error=pca.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=1 # 2 CPUs per job

module load plink

cd /global/scratch/users/rdekayne/gorilla_census/03_PCA

plink --vcf /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_X_Y_mindepth7_minqual30_mac1_AN28_AN22_AN6.vcf.gz --double-id --allow-extra-chr --set-missing-var-ids @:# --make-bed --pca --out 14gorillas_unfilt_out

touch /global/scratch/users/rdekayne/gorilla_census/03_PCA/14_pca.done

##run
sbatch 03.1_PCA_unfilt.sh

#  3823659 variants and 14 people pass filters and QC.

scp rdekayne@hpc.brc.berkeley.edu:/global/scratch/users/rdekayne/gorilla_census/03_PCA/*.eigen* .

#####################################
#####################################
#            04_geno
#####################################
#####################################

#get genomics_general here
git clone https://github.com/simonhmartin/genomics_general

mkdir -p /global/scratch/users/rdekayne/gorilla_census/04_geno && cd /global/scratch/users/rdekayne/gorilla_census/04_geno
mkdir -p /global/scratch/users/rdekayne/gorilla_census/02_genotyping/genos

ls /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_vcfs/*AN28.vcf.gz > filt_vcfs_to_convert.txt
sed -i 's/\/global\/scratch\/users\/rdekayne\/gorilla_census\/02_genotyping\/filt_vcfs\///g' filt_vcfs_to_convert.txt
sed -i 's/_filt_mindepth7_minqual30_m2M2_AN28.vcf.gz//g' filt_vcfs_to_convert.txt

# 04.1_geno_convert.sh

#!/bin/bash
#SBATCH --job-name=geno
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=geno.%j.out # output file
#SBATCH --error=geno.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=4 # 2 CPUs per job

module load python/3.11.6-gcc-11.4.0 bio/bcftools/1.16-gcc-11.4.0

numb=${SLURM_ARRAY_TASK_ID}
scaf_name=$(cat /global/scratch/users/rdekayne/gorilla_census/02_genotyping/autosomes_scaffold_list.txt | sed -n ${numb}p)

mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_"${scaf_name}"

bcftools filter -Ou /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_reheader_vcfs/"${scaf_name}".reheader.raw.vcf.gz --set-GTs . -e 'AN < 2' | bcftools filter -e 'INFO/MAC<1 | AN<28' | bcftools view -m1 -M2 -v snps | bcftools filter -e '(FORMAT/DP)<7 | (FORMAT/GQ)<30' | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_temp_"${scaf_name}" -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/genos/"${scaf_name}"_filt_mindepth7_minqual30_m1M2_AN28.vcf.gz 

python /global/scratch/users/rdekayne/gorilla_census/04_geno/genomics_general/VCF_processing/parseVCF.py -i /global/scratch/users/rdekayne/gorilla_census/02_genotyping/genos/"${scaf_name}"_filt_mindepth7_minqual30_m1M2_AN28.vcf.gz --skipIndels --minQual 30 --gtf flag=DP min=7 max=100 -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/genos/"${scaf_name}"_filt_mindepth7_minqual30_m1M2_AN28.geno.gz

touch /global/scratch/users/rdekayne/gorilla_census/02_genotyping/genos/"${scaf_name}".done

##run
sbatch --array=1-23 04.1_geno_convert.sh

cat ../02_genotyping/X_ploidy.txt | sed 's/\/global\/scratch\/users\/rdekayne\/gorilla_census\/01_mapping\/indiv_bams\///g' | sed 's/_1file.bam//g' > x_geno_ploidy.txt
cat ../02_genotyping/Y_ploidy.txt | sed 's/\/global\/scratch\/users\/rdekayne\/gorilla_census\/01_mapping\/indiv_bams\///g' | sed 's/_1file.bam//g' > y_geno_ploidy.txt
#error with y so put ploidy to 1

## now sex chromosomes
# 04.2_geno_convert_sex.sh

#!/bin/bash
#SBATCH --job-name=geno
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=geno.%j.out # output file
#SBATCH --error=geno.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=4 # 2 CPUs per job

module load python/3.11.6-gcc-11.4.0 bio/bcftools/1.16-gcc-11.4.0

python /global/scratch/users/rdekayne/gorilla_census/04_geno/genomics_general/VCF_processing/parseVCF.py -i /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_vcfs/chrX_mat_hsaX_filt_mindepth7_minqual30_m2M2_AN22.vcf.gz --skipIndels --minQual 30 --gtf flag=DP min=7 max=100 --ploidyFile x_geno_ploidy.txt -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/genos/chrX_mat_hsaX_filt_mindepth7_minqual30_m2M2_AN22.geno.gz

python /global/scratch/users/rdekayne/gorilla_census/04_geno/genomics_general/VCF_processing/parseVCF.py -i /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_vcfs/chrY_pat_hsaY_mindepth7_minqual30_m2M2_AN6.vcf.gz --skipIndels --minQual 30 --gtf flag=DP min=7 max=100 --ploidyFile y_all1_geno_ploidy.txt -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/genos/chrY_pat_hsaY_mindepth7_minqual30_m2M2_AN6.geno.gz

touch /global/scratch/users/rdekayne/gorilla_census/02_genotyping/genos/sex.done

##run
sbatch 04.2_geno_convert_sex.sh


##gorilla_popsfile.txt
ind01_Maisha  mount
ind02_Tuck  mount
ind03_Turimaso  mount
ind04_Umurimo  mount
ind05_Imfura  mount
ind06_Kaboko  mount
ind07_Zirikana  mount
ind08_Dunia  elow
ind09_Itebero  elow
ind10_Pinga  elow
ind12_Tumani  elow
ind13_Ntabwoba  elow
ind14_Mkubwa  elow
ind15_Kaisi  elow

#now for analysis
#now calculate pi in a loop
#genomics.py needs to be here:

# 04.3_geno_pi.sh

#!/bin/bash
#SBATCH --job-name=geno
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=geno.%j.out # output file
#SBATCH --error=geno.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=4 # 2 CPUs per job

module load python/3.11.6-gcc-11.4.0 bio/bcftools/1.16-gcc-11.4.0

numb=${SLURM_ARRAY_TASK_ID}
scaf_name=$(cat /global/scratch/users/rdekayne/gorilla_census/04_geno/filt_vcfs_to_convert.txt | sed -n ${numb}p)

python /global/scratch/users/rdekayne/gorilla_census/04_geno/genomics_general/popgenWindows.py -w 50000 -m 2000 -g /global/scratch/users/rdekayne/gorilla_census/02_genotyping/genos/"${scaf_name}"_filt_mindepth7_minqual30_AN28.geno.gz -o "${scaf_name}".div.regions.output.csv.gz -f phased -T 1 -p mount -p elow --popsFile /global/scratch/users/rdekayne/gorilla_census/04_geno/gorilla_popsfile.txt

touch /global/scratch/users/rdekayne/gorilla_census/04_geno/"${scaf_name}".pi.regions.done

##run
sbatch --array=1-23 04.3_geno_pi.sh

##leaving these out but could calculate the same metrics across these regions
#python /global/scratch/users/rdekayne/gorilla_census/04_geno/genomics_general/popgenWindows.py -w 50000 -m 5000 -g /global/scratch/users/rdekayne/gorilla_census/02_genotyping/genos/chrX_mat_hsaX_filt_mindepth7_minqual30_mac1_AN22.geno.gz -o chrX_mat_hsaX.div.regions.output.csv.gz -f phased -T 1 --ploidyFile x_geno_ploidy.txt -p mount -p elow --popsFile /global/scratch/users/rdekayne/gorilla_census/04_geno/gorilla_popsfile.txt

#python /global/scratch/users/rdekayne/gorilla_census/04_geno/genomics_general/popgenWindows.py -w 50000 -m 5000 -g /global/scratch/users/rdekayne/gorilla_census/02_genotyping/genos/chrY_pat_hsaY_filt_mindepth7_minqual30_mac1_AN6.geno.gz -o chrY_pat_hsaY.div.regions.output.csv.gz -f phased -T 1 --ploidyFile y_all1_geno_ploidy.txt -p mount -p elow --popsFile /global/scratch/users/rdekayne/gorilla_census/04_geno/gorilla_popsfile.txt

mkdir -p /global/scratch/users/rdekayne/gorilla_census/04_geno/pi_output
cp *.div.regions.output.csv.gz /global/scratch/users/rdekayne/gorilla_census/04_geno/pi_output

scp rdekayne@hpc.brc.berkeley.edu:/global/scratch/users/rdekayne/gorilla_census/04_geno/pi_output/*.csv.gz .


#####################################
#####################################
#            05_mpcrselect
#####################################
#####################################

mkdir -p /global/scratch/users/rdekayne/gorilla_census/05_mpcrselect && cd /global/scratch/users/rdekayne/gorilla_census/05_mpcrselect

#https://github.com/ellieearmstrong/mPCRselect/tree/main
#install dependencies

#install nextflow
module load openjdk/17.0.8.1_1-gcc-11.4.0
curl -s https://get.nextflow.io | bash

#install pipeline
nextflow pull ellieearmstrong/mPCRselect -r main

#Clone the NGS-PrimerPlex repository
git clone https://github.com/aakechin/NGS-PrimerPlex

#Install the Python dependencies: 
cd NGS-PrimerPlex/
bash install_for_linux.sh 
chmod +x NGS_primerplex.py
#Modify the shebang line of the NGS_primerplex.py script: Change the first line from #!/usr/bin/python3 to #!/usr/bin/env python3
#Move the NGS_primerplex.py script to a directory in your PATH variable
cp NGS_primerplex.py /global/scratch/users/rdekayne/gorilla_census/

git clone https://github.com/ellieearmstrong/mPCRselect
cp mPCRselect/nextflow.config test.config

conda create -p /global/scratch/users/rdekayne/envs/mpcrselectenv
conda activate /global/scratch/users/rdekayne/envs/mpcrselectenv
conda install bioconda::bwa=0.7.17 
conda install conda-forge::libzlib=1.2.13 
conda install conda-forge::ruby=3.2.2 
conda install bioconda::vcftools=0.1.16 
conda install bioconda::plink2=2.00a5.10 
conda install bioconda::bcftools=1.18 
conda install conda-forge::gsl=2.7 
conda install conda-forge::gzip 
conda install conda-forge::gawk 
conda install conda-forge::r-tidyverse=1.3.1 
conda install conda-forge::r-caret 
conda install conda-forge::r-ggplot2=3.4.0
conda install bioconda::bedtools=2.31.0 
conda install -c conda-forge r-base=4.2.3
conda install bioconda::primer3-py
conda install conda-forge::biopython
conda install bioconda::pysam
conda install conda-forge::xlrd
conda install conda-forge::xlsxwriter
pip install networkx==1.11
#had to downgrade python:   python                pkgs/main::python-3.9.20-he870216_1 --> conda-forge::python-3.7.12-hf930737_100_cpython 
conda install python=3.7

git clone https://github.com/campanam/baitstools
cd baitstools
gem build baitstools.gemspec
gem install baitstools-1.8.1.gem

##check dependency versions match github
cat sample.csv 
Sample,Population
ind01_Maisha,mont
ind02_Tuck,mont
ind03_Turimaso,mont
ind04_Umurimo,mont
ind05_Imfura,mont
ind06_Kaboko,mont
ind07_Zirikana,mont
ind08_Dunia,elow
ind09_Itebero,elow
ind10_Pinga,elow
ind12_Tumani,elow
ind13_Ntabwoba,elow
ind14_Mkubwa,elow
ind15_Kaisi,elow

#edit test.config

cp /global/scratch/users/rdekayne/gorilla_census/data/genomes/mGorGor1.pri.cur.20231122.fasta ./gorilla.ref.fasta
cp /global/scratch/users/rdekayne/gorilla_census/data/genomes/mGorGor1.pri.cur.20231122.fasta.fai ./gorilla.ref.fasta.fai

##reheader_fasta.sh

#!/bin/bash
#SBATCH --job-name=rehead
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=rehead.%j.out # output file
#SBATCH --error=rehead.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=4 # 2 CPUs per job

sed -i 's/chr1_pat_hsa1/chr1pathsa1/g' ./gorilla.ref.fasta;
sed -i 's/chr2_pat_hsa3/chr2pathsa3/g' ./gorilla.ref.fasta;
sed -i 's/chr3_pat_hsa4/chr3pathsa4/g' ./gorilla.ref.fasta;
sed -i 's/chr4_pat_hsa17x5/chr4pathsa17x5/g' ./gorilla.ref.fasta;
sed -i 's/chr5_mat_hsa6/chr5mathsa6/g' ./gorilla.ref.fasta;
sed -i 's/chr6_mat_hsa7/chr6mathsa7/g' ./gorilla.ref.fasta;
sed -i 's/chr7_pat_hsa8/chr7pathsa8/g' ./gorilla.ref.fasta;
sed -i 's/chr8_pat_hsa10/chr8pathsa10/g' ./gorilla.ref.fasta;
sed -i 's/chr9_pat_hsa11/chr9pathsa11/g' ./gorilla.ref.fasta;
sed -i 's/chr10_mat_hsa12/chr10mathsa12/g' ./gorilla.ref.fasta;
sed -i 's/chr11_mat_hsa2b/chr11mathsa2b/g' ./gorilla.ref.fasta;
sed -i 's/chr12_pat_hsa2a/chr12pathsa2a/g' ./gorilla.ref.fasta;
sed -i 's/chr13_pat_hsa9/chr13pathsa9/g' ./gorilla.ref.fasta;
sed -i 's/chr14_pat_hsa13/chr14pathsa13/g' ./gorilla.ref.fasta;
sed -i 's/chr15_pat_hsa14/chr15pathsa14/g' ./gorilla.ref.fasta;
sed -i 's/chr16_pat_hsa15/chr16pathsa15/g' ./gorilla.ref.fasta;
sed -i 's/chr17_mat_hsa18/chr17mathsa18/g' ./gorilla.ref.fasta;
sed -i 's/chr18_pat_hsa16/chr18pathsa16/g' ./gorilla.ref.fasta;
sed -i 's/chr19_pat_hsa5x17/chr19pathsa5x17/g' ./gorilla.ref.fasta;
sed -i 's/chr20_mat_hsa19/chr20mathsa19/g' ./gorilla.ref.fasta;
sed -i 's/chr21_pat_hsa20/chr21pathsa20/g' ./gorilla.ref.fasta;
sed -i 's/chr22_mat_hsa21/chr22mathsa21/g' ./gorilla.ref.fasta;
sed -i 's/chr23_mat_hsa22/chr23mathsa22/g' ./gorilla.ref.fasta;
done

sed -i 's/chr1_pat_hsa1/chr1pathsa1/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr2_pat_hsa3/chr2pathsa3/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr3_pat_hsa4/chr3pathsa4/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr4_pat_hsa17x5/chr4pathsa17x5/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr5_mat_hsa6/chr5mathsa6/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr6_mat_hsa7/chr6mathsa7/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr7_pat_hsa8/chr7pathsa8/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr8_pat_hsa10/chr8pathsa10/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr9_pat_hsa11/chr9pathsa11/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr10_mat_hsa12/chr10mathsa12/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr11_mat_hsa2b/chr11mathsa2b/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr12_pat_hsa2a/chr12pathsa2a/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr13_pat_hsa9/chr13pathsa9/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr14_pat_hsa13/chr14pathsa13/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr15_pat_hsa14/chr15pathsa14/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr16_pat_hsa15/chr16pathsa15/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr17_mat_hsa18/chr17mathsa18/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr18_pat_hsa16/chr18pathsa16/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr19_pat_hsa5x17/chr19pathsa5x17/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr20_mat_hsa19/chr20mathsa19/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr21_pat_hsa20/chr21pathsa20/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr22_mat_hsa21/chr22mathsa21/g' ./gorilla.ref.fasta.fai;
sed -i 's/chr23_mat_hsa22/chr23mathsa22/g' ./gorilla.ref.fasta.fai;
done

##run
sbatch reheader_fasta.sh

###now prep mpcrselect submission config and submission script
cp test.config run_one.config
mkdir run_one_output

cp run_one.config run_two.config
mkdir run_two_output

# submit_run_two_mpcrselect.sh 

#!/bin/bash
#SBATCH --job-name=mpcr
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio4_htc
#SBATCH --qos=genomicdata_htc4_normal
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=20
#SBATCH --mem=100G
#SBATCH --time=12:00:00
#SBATCH --output=mpcr.%j.out
#SBATCH --error=mpcr.%j.err

module load openjdk/17.0.8.1_1-gcc-11.4.0

nextflow run ellieearmstrong/mPCRselect -r main -c run_two.config

##run
sbatch submit_run_two_mpcrselect.sh 

#####################################
#####################################
#            06_snp_panel_popgen
#####################################
#####################################

mkdir -p /global/scratch/users/rdekayne/gorilla_census/06_snp_panel_popgen && cd /global/scratch/users/rdekayne/gorilla_census/06_snp_panel_popgen

conda activate /global/scratch/users/rdekayne/envs/pca

## 06_PCA.sh
#!/bin/bash
#SBATCH --job-name=pca
#SBATCH --time=0-4:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=pca.%j.out # output file
#SBATCH --error=pca.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=1 # 2 CPUs per job

cd /global/scratch/users/rdekayne/gorilla_census/06_snp_panel_popgen

plink --vcf /global/scratch/users/rdekayne/gorilla_census/05_mpcrselect/run_two_output/17_Fst_Pi_SNPs/14gorilla_autosomes_mindepth7_minqual30_m2M2_AN28_chrrename.fst_pi.vcf.gz --double-id --allow-extra-chr --set-missing-var-ids @:# --make-bed --pca --out 14gorillas_2000snp_panel_unfilt_out

touch ./14_2000snp.done

##run
sbatch 06_PCA.sh

scp rdekayne@hpc.brc.berkeley.edu:/global/scratch/users/rdekayne/gorilla_census/06_snp_panel_popgen/*.eigen* .


conda create -p /global/scratch/users/rdekayne/envs/vcftools
conda activate /global/scratch/users/rdekayne/envs/vcftools
conda install bioconda/label/cf201901::vcftools

#mont_indivs.txt
ind01_Maisha
ind02_Tuck
ind03_Turimaso
ind04_Umurimo
ind05_Imfura
ind06_Kaboko
ind07_Zirikana

#elow_indivs.txt
ind08_Dunia
ind09_Itebero
ind10_Pinga
ind12_Tumani
ind13_Ntabwoba
ind14_Mkubwa
ind15_Kaisi

vcftools --gzvcf /global/scratch/users/rdekayne/gorilla_census/05_mpcrselect/run_two_output/17_Fst_Pi_SNPs/14gorilla_autosomes_mindepth7_minqual30_m2M2_AN28_chrrename.fst_pi.vcf.gz --weir-fst-pop mont_indivs.txt --weir-fst-pop elow_indivs.txt --out fst_snp2000

#After filtering, kept 14 out of 14 Individuals
Outputting Weir and Cockerham Fst estimates.
Weir and Cockerham mean Fst estimate: 0.49082
Weir and Cockerham weighted Fst estimate: 0.60697
After filtering, kept 2000 out of a possible 2000 Sites
Run Time = 0.00 seconds

#####################################
#####################################
#            07_NGS_primer
#####################################
#####################################

mkdir -p /global/scratch/users/rdekayne/gorilla_census/07_NGS_primer && cd /global/scratch/users/rdekayne/gorilla_census/07_NGS_primer
#use mpcrselect conda env 
conda activate /global/scratch/users/rdekayne/envs/mpcrselectenv

#then manually run ngsprimerplex on 500 snps










#####################################
#####################################
#            OLD
#####################################
#####################################

## 02.5_vcf_filt.sh

#!/bin/bash
#SBATCH --job-name=filt
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=filt.%j.out # output file
#SBATCH --error=filt.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=4 # 2 CPUs per job

numb=${SLURM_ARRAY_TASK_ID}
scaf_name=$(cat /global/scratch/users/rdekayne/gorilla_census/02_genotyping/autosomes_scaffold_list.txt | sed -n ${numb}p)

mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_"${scaf_name}"

bcftools filter -Ou /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_reheader_vcfs/"${scaf_name}".reheader.raw.vcf.gz --set-GTs . -e 'AN < 2' | bcftools filter -e 'INFO/MAC<1 | AN<28' | bcftools view -m2 -M2 -v snps | bcftools filter -e '(FORMAT/DP)<7 | (FORMAT/GQ)<30' | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_temp_"${scaf_name}" -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_vcfs/"${scaf_name}"_filt_mindepth7_minqual30_m2M2_AN28.vcf.gz && touch /global/scratch/users/rdekayne/gorilla_census/02_genotyping/done_files/"${scaf_name}"_filt.done

##run
sbatch --array=1-23 02.5_vcf_filt.sh

## 02.6_vcf_filt_sex.sh

#!/bin/bash
#SBATCH --job-name=filt
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=filt.%j.out # output file
#SBATCH --error=filt.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=4 # 2 CPUs per job

mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filtX

bcftools filter -Ou /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_reheader_vcfs/chrX_mat_hsaX.reheader.raw.vcf.gz --set-GTs . -e 'AN < 2' | bcftools filter -e 'INFO/MAC<1 | AN<22' | bcftools view -m2 -M2 -v snps | bcftools filter -e '(FORMAT/DP)<7 | (FORMAT/GQ)<30' | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filtX -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_vcfs/chrX_mat_hsaX_filt_mindepth7_minqual30_m2M2_AN22.vcf.gz && touch /global/scratch/users/rdekayne/gorilla_census/02_genotyping/done_files/chrX_mat_hsaX_filt.done

mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filtY

bcftools filter -Ou /global/scratch/users/rdekayne/gorilla_census/02_genotyping/raw_reheader_vcfs/chrY_pat_hsaY.reheader.raw.vcf.gz --set-GTs . -e 'AN < 2' | bcftools filter -e 'INFO/MAC<1 | AN<6' | bcftools view -m2 -M2 -v snps | bcftools filter -e '(FORMAT/DP)<7 | (FORMAT/GQ)<30' | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filtY -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_vcfs/chrY_pat_hsaY_mindepth7_minqual30_m2M2_AN6.vcf.gz && touch /global/scratch/users/rdekayne/gorilla_census/02_genotyping/done_files/chrY_pat_hsaY_filt.done

##run
sbatch 02.6_vcf_filt_sex.sh


## 02.7_vcf_concat_filt.sh

#!/bin/bash
#SBATCH --job-name=conc
#SBATCH --time=0-24:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --account=co_genomicdata
#SBATCH --partition=savio3
#SBATCH --output=conc.%j.out # output file
#SBATCH --error=conc.%j.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=4 # 2 CPUs per job

mkdir -p /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs
mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat
mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat_filt

ls /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_vcfs/*AN28.vcf.gz > autosomes_to_merge.txt

bcftools concat -Ou -f autosomes_to_merge.txt | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concat -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_mindepth7_minqual30_m2M2_AN28.vcf.gz

ls /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_mindepth7_minqual30_m2M2_AN28.vcf.gz > auto_sex_vcf_merge.txt
ls /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_vcfs/chrX_mat_hsaX_filt_mindepth7_minqual30_m2M2_AN22.vcf.gz >> auto_sex_vcf_merge.txt
ls /global/scratch/users/rdekayne/gorilla_census/02_genotyping/filt_vcfs/chrY_pat_hsaY_mindepth7_minqual30_m2M2_AN6.vcf.gz >> auto_sex_vcf_merge.txt

mkdir -p /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concatall
bcftools concat -Ou -f auto_sex_vcf_merge.txt | bcftools sort -Oz --temp-dir /global/scratch/users/rdekayne/gorilla_census/genotype_temp_concatall -o /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_X_Y_mindepth7_minqual30_mac1_AN28_AN22_AN6.vcf.gz

bcftools view -H /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_mindepth7_minqual30_m2M2_AN28.vcf.gz | wc -l
bcftools view -H /global/scratch/users/rdekayne/gorilla_census/02_genotyping/concat_vcfs/14gorilla_autosomes_X_Y_mindepth7_minqual30_mac1_AN28_AN22_AN6.vcf.gz | wc -l

touch genotype_stats.done

